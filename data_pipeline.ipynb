{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from functools import reduce\n",
    "\n",
    "import ee\n",
    "from ismn.interface import ISMN_Interface\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authenticate the library using your Google account\n",
    "ee.Authenticate()\n",
    "\n",
    "# Initializes the Google Earth Engine library\n",
    "ee.Initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cloud masking steps for Sentinel-2 images ##\n",
    "\n",
    "# Define cloud mask parameters\n",
    "CLOUD_FILTER = 80 # maximum percent of cloud cover in the image\n",
    "CLD_PRB_THRESH = 50 # probability of clouds at pixel\n",
    "NIR_DRK_THRESH = 0.15 # NIR reflectance; values below threshold are considered clouds\n",
    "CLD_PRJ_DIST = 1 # maximum distance to search for cloud shadows from cloud edges\n",
    "BUFFER = 50 # dilation distance from edge of cloud objects\n",
    "\n",
    "## Creates an image collection from specified parameters of cloud probability\n",
    "def get_s2_sr_cld_col(start_date, end_date):\n",
    "    # Import and filter S2 SR.\n",
    "    s2_sr_col = (ee.ImageCollection('COPERNICUS/S2_SR')\n",
    "        .filterDate(start_date, end_date)\n",
    "        .filter(ee.Filter.lte('CLOUDY_PIXEL_PERCENTAGE', CLOUD_FILTER)))\n",
    "\n",
    "    # Import and filter s2cloudless.\n",
    "    s2_cloudless_col = (ee.ImageCollection('COPERNICUS/S2_CLOUD_PROBABILITY')\n",
    "        .filterDate(start_date, end_date))\n",
    "\n",
    "    # Join the filtered s2cloudless collection to the SR collection by the 'system:index' property.\n",
    "    return ee.ImageCollection(ee.Join.saveFirst('s2cloudless').apply(**{\n",
    "        'primary': s2_sr_col,\n",
    "        'secondary': s2_cloudless_col,\n",
    "        'condition': ee.Filter.equals(**{\n",
    "            'leftField': 'system:index',\n",
    "            'rightField': 'system:index'\n",
    "        })\n",
    "    }))\n",
    "\n",
    "## Cloud component\n",
    "## Adds the s2cloudless probability layers and derives a cloud mask\n",
    "def add_cloud_bands(img):\n",
    "    # Get s2cloudless image, subset the probability band.\n",
    "    cld_prb = ee.Image(img.get('s2cloudless')).select('probability')\n",
    "\n",
    "    # Condition s2cloudless by the probability threshold value.\n",
    "    is_cloud = cld_prb.gt(CLD_PRB_THRESH).rename('clouds')\n",
    "\n",
    "    # Add the cloud probability layer and cloud mask as image bands.\n",
    "    return img.addBands(ee.Image([cld_prb, is_cloud]))\n",
    "\n",
    "## Cloud shadow component\n",
    "## Adds dark pixels, cloud projection, and identified shadows as bands to the image collection\n",
    "def add_shadow_bands(img):\n",
    "    # Identify water pixels from the SCL band.\n",
    "    not_water = img.select('SCL').neq(6)\n",
    "\n",
    "    # Identify dark NIR pixels that are not water (potential cloud shadow pixels).\n",
    "    SR_BAND_SCALE = 1e4\n",
    "    dark_pixels = img.select('B8').lt(NIR_DRK_THRESH*SR_BAND_SCALE).multiply(not_water).rename('dark_pixels')\n",
    "\n",
    "    # Determine the direction to project cloud shadow from clouds (assumes UTM projection).\n",
    "    shadow_azimuth = ee.Number(90).subtract(ee.Number(img.get('MEAN_SOLAR_AZIMUTH_ANGLE')));\n",
    "\n",
    "    # Project shadows from clouds for the distance specified by the CLD_PRJ_DIST input.\n",
    "    cld_proj = (img.select('clouds').directionalDistanceTransform(shadow_azimuth, CLD_PRJ_DIST*10)\n",
    "        .reproject(**{'crs': img.select(0).projection(), 'scale': 100})\n",
    "        .select('distance')\n",
    "        .mask()\n",
    "        .rename('cloud_transform'))\n",
    "\n",
    "    # Identify the intersection of dark pixels with cloud shadow projection.\n",
    "    shadows = cld_proj.multiply(dark_pixels).rename('shadows')\n",
    "\n",
    "    # Add dark pixels, cloud projection, and identified shadows as image bands.\n",
    "    return img.addBands(ee.Image([dark_pixels, cld_proj, shadows]))\n",
    "\n",
    "## Assembles the cloud-shadow mask to produce a final masking of the images\n",
    "def add_cld_shdw_mask(img):\n",
    "    # Add cloud component bands.\n",
    "    img_cloud = add_cloud_bands(img)\n",
    "\n",
    "    # Add cloud shadow component bands.\n",
    "    img_cloud_shadow = add_shadow_bands(img_cloud)\n",
    "\n",
    "    # Combine cloud and shadow mask, set cloud and shadow as value 1, else 0.\n",
    "    is_cld_shdw = img_cloud_shadow.select('clouds').add(img_cloud_shadow.select('shadows')).gt(0)\n",
    "\n",
    "    # Remove small cloud-shadow patches and dilate remaining pixels by BUFFER input.\n",
    "    # 20 m scale is for speed, and assumes clouds don't require 10 m precision.\n",
    "    is_cld_shdw = (is_cld_shdw.focal_min(2).focal_max(BUFFER*2/20)\n",
    "        .reproject(**{'crs': img.select([0]).projection(), 'scale': 20})\n",
    "        .rename('cloudmask'))\n",
    "\n",
    "    # Add the final cloud-shadow mask to the image.\n",
    "    return img.addBands(is_cld_shdw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Converts an EarthEngine image collection into a Pandas array for a given point\n",
    "## with the specified bands.\n",
    "## The buffer attribute should be equal to half the spatial resolution of the final product.\n",
    "## The bands should be given as a list, even for single bands.\n",
    "def ee_to_df(ee_arr, lon, lat, buffer, int_limit, bands, start_date, end_date):\n",
    "    # Converts columns to numeric values\n",
    "    def to_numeric(dataframe, band):\n",
    "        dataframe[band] = pd.to_numeric(dataframe[band], errors='coerce')\n",
    "        return dataframe\n",
    "\n",
    "    # Transform the client-side data to a dataframe\n",
    "    poi = ee.Geometry.Point(lon, lat)\n",
    "    arr = ee_arr.select(bands).getRegion(poi, buffer).getInfo()\n",
    "    df = pd.DataFrame(arr)\n",
    "    headers = df.iloc[0]\n",
    "    df = pd.DataFrame(df.values[1:], columns=headers)\n",
    "\n",
    "    # Applies the to_numeric function and fills NaN rows with interpolated values \n",
    "    for band in bands:\n",
    "        df = to_numeric(df, band)\n",
    "        if int_limit > 0:\n",
    "            df[band].interpolate(method='linear', limit=int_limit, limit_direction='both',\n",
    "                                 inplace=True)\n",
    "        df.drop_duplicates(keep='first') # remove duplicates\n",
    "\n",
    "    # Creates an index date column and drops unnecessary date, time, and coordinate columns\n",
    "    df['Date'] = pd.to_datetime(df['time'], unit='ms')\n",
    "    df['Date'] = df['Date'].dt.date\n",
    "    df.set_index('Date', inplace=True)\n",
    "    df.drop(['id', 'time', 'longitude', 'latitude'], axis=1, inplace=True)\n",
    "    \n",
    "    # Drop duplicate entries from the index and reindex the dataset to daily timesteps\n",
    "    df = df[~df.index.duplicated()]\n",
    "    df = df.reindex(pd.date_range(start=start_date, end=end_date, freq='D'))\n",
    "    df.index.name = 'Date'\n",
    "    \n",
    "    return df\n",
    "\n",
    "## Creates a new column that counts the number of days between precipitation events\n",
    "def dry_days(df):\n",
    "    dry_streak = []\n",
    "    counter = 0\n",
    "    for day, value in enumerate(df['total_precipitation']):\n",
    "        if value > 0:\n",
    "            dry_streak.append(0)\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "            dry_streak.append(counter)\n",
    "    df['Dry_days'] = dry_streak\n",
    "    df['Dry_days'] = df['Dry_days'].astype('float')\n",
    "    return df\n",
    "\n",
    "## Calculates and adds the vegetation index profiles from the S2 data.\n",
    "## The indices are interpolated linearly 30 days in both directions.\n",
    "## NDWI is based on https://doi.org/10.1016/S0034-4257(96)00067-3\n",
    "def add_vegetation_index(df, name, band_1, band_2):\n",
    "    df[name] = (df[band_1] - df[band_2]) / (df[band_1] + df[band_2])\n",
    "    df[name].interpolate(method='linear', limit=30, limit_direction='both', inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compiles a dataframes of all remotely sensed EE variables at a given point\n",
    "def get_rs_data(lon, lat, point_id):\n",
    "    # MODIS\n",
    "    lst = ee_to_df(land_t, lon, lat, 5, 5, ['LST_Day_1km'], start_date, end_date)\n",
    "    lst = lst * 0.02 - 273.15 # scaling factor and Kelvin to Celcius conversion \n",
    "\n",
    "    # Sentinel-1\n",
    "    sigma = ee_to_df(s1, lon, lat, 5, 0, ['VV', 'VH', 'angle'], start_date, end_date)\n",
    "\n",
    "    # Sentinel-2\n",
    "    s2_vi = ee_to_df(s2, lon, lat, 5, 0, ['B4', 'B8', 'B8A', 'B11'], start_date, end_date)\n",
    "    s2_vi = add_vegetation_index(s2_vi, 'NDVI', 'B4', 'B8')\n",
    "    s2_vi = add_vegetation_index(s2_vi, 'NDWI', 'B8A', 'B11')\n",
    "    s2_vi.drop(['B4', 'B8', 'B8A', 'B11'], axis=1, inplace=True)\n",
    "\n",
    "    # ERA-5\n",
    "    era5_weather = ee_to_df(era5, lon, lat, 5, 0, ['mean_2m_air_temperature', 'total_precipitation'],\n",
    "                 start_date, end_date)\n",
    "    era5_weather = dry_days(era5_weather)\n",
    "    era5_weather['mean_2m_air_temperature'] = era5_weather['mean_2m_air_temperature'] - 273.15\n",
    "\n",
    "    # Merge dataframes\n",
    "    dfs = [lst, sigma, s2_vi, era5_weather]\n",
    "    df = reduce(lambda  left,right: pd.merge(left, right, on=['Date'], how='outer'), dfs)\n",
    "    df['ID'] = point_id\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "## Creates a dataframe from ISMN data \n",
    "## Using the ISMN reader https://github.com/TUW-GEO/ismn\n",
    "def get_sm_data(network, depth_start, depth_end, snow_depth=False):\n",
    "    # Retrieves the name of each station in the network\n",
    "    stations = {}\n",
    "    for n, station in enumerate(sm_data[network]):\n",
    "        stations[n] = station.metadata['station'][1]\n",
    "    \n",
    "    # Creates a new dataframe with the station IDs and coordinates\n",
    "    grid = sm_data.collection.grid\n",
    "    gpis, lon, lat = grid.get_grid_points()\n",
    "    df_coords = pd.DataFrame(index=pd.Index(gpis, name='ID'), data={'longitude': lon,\n",
    "                                                                    'latitude': lat})\n",
    "    df_coords = df_coords.rename(index=stations)\n",
    "\n",
    "    # Writes soil moisture at 0-5cm depth to a dataframe\n",
    "    sm_df = pd.DataFrame()\n",
    "    for network, station, sensor in sm_data.collection.iter_sensors(\n",
    "        variable='soil_moisture', depth=[depth_start, depth_end]):\n",
    "        \n",
    "        data = sensor.read_data()\n",
    "        sensor_df = pd.DataFrame(data)\n",
    "        sensor_df = sensor_df[sensor_df['soil_moisture_flag'] == 'G'] # filters out bad readings\n",
    "        sensor_df = sensor_df.resample('D').mean() # resamples to daily values\n",
    "        sensor_df['Date'] = sensor_df.index.date\n",
    "        sensor_df['ID'] = station.metadata['station'][1]\n",
    "        sm_df = sm_df.append(sensor_df)\n",
    "    \n",
    "    # Writes snow depth to a dataframe if specified and filters out days with snow cover\n",
    "    if snow_depth:\n",
    "        snow_df = pd.DataFrame()\n",
    "        for network, station, sensor in sm_data.collection.iter_sensors(\n",
    "            variable='snow_depth', depth=[0., 0.]):\n",
    "            \n",
    "            data = sensor.read_data()\n",
    "            sensor_df = pd.DataFrame(data)\n",
    "            sensor_df.loc[sensor_df['snow_depth']<=0, 'snow'] = 'yes'\n",
    "            sensor_df = sensor_df.resample('D').count() # counts the entries with snow cover\n",
    "            sensor_df['Date'] = sensor_df.index.date\n",
    "            sensor_df['ID'] = station.metadata['station'][1]\n",
    "            sensor_df = sensor_df[['ID', 'Date', 'snow']]\n",
    "            snow_df = snow_df.append(sensor_df)\n",
    "        try:\n",
    "            df = pd.merge(sm_df, snow_df, how='left', left_on=['Date', 'ID'], right_on=['Date', 'ID'])\n",
    "            df = df[df['snow']==0] # any day with snow cover entries is discarded\n",
    "        except KeyError:\n",
    "            print('No snow depth variable found, set snow_depth=False and run again')\n",
    "    else:\n",
    "        df = sm_df\n",
    "        \n",
    "    df.rename({'Date_x' : 'Date'}, axis=1, inplace=True)\n",
    "    df.set_index('Date', inplace=True)\n",
    "    df = df[['ID', 'soil_moisture']]\n",
    "    print('ISMN data successfully translated to dataframe')\n",
    "    return df, df_coords\n",
    "\n",
    "\n",
    "## Queries the Earth Engine data for each soil moisture probe (lat, lon coordinates) in the\n",
    "## dataset and merges the EE data with the sm data\n",
    "def merge_sm_rs_data(soil_moisture_df, coordinate_df):\n",
    "    df = pd.DataFrame()\n",
    "    print('Extracting data from Earth Engine...')\n",
    "    i_start, i_end = 1, len(coordinate_df)\n",
    "    for station, coords in coordinate_df.iterrows():\n",
    "        print('Station {} of {}'.format(i_start, i_end))\n",
    "        df_sm_temp = soil_moisture_df[soil_moisture_df['ID'] == station]\n",
    "        df_rs_temp = (get_rs_data(coords['longitude'], coords['latitude'], station))\n",
    "        df_rs_temp = df_rs_temp.reindex(pd.date_range(start=start_date, end=end_date, freq='D'))\n",
    "        df_temp = pd.merge(df_rs_temp, df_sm_temp, how='left', left_index=True, right_index=True)\n",
    "        df_temp.rename({'ID_x' : 'ID'}, axis=1, inplace=True)\n",
    "        df_temp.drop('ID_y', axis=1, inplace=True)\n",
    "        df = df.append(df_temp)\n",
    "        i_start += 1\n",
    "    df.index.name = 'Date'\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data specifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!# Important to verify that all datasets contain data between the two periods #!#\n",
    "#!# The current timeframe between start and end are the lower and upper limits, #!#\n",
    "#!# defined by the start of Sentinel-2 SR data and end of ERA-5\n",
    "start_date, end_date = '2017-03-28', '2020-07-09' \n",
    "\n",
    "# Sentinel-1 GRD C-band SAR data\n",
    "# https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_S1_GRD\n",
    "# Values are expressed in decibel on a logarithmic scale\n",
    "s1 = (ee.ImageCollection(\"COPERNICUS/S1_GRD\")\n",
    "      .filterDate(ee.Date(start_date), ee.Date(end_date))\n",
    "      .filter(ee.Filter.listContains('transmitterReceiverPolarisation', 'VV'))\n",
    "      .filter(ee.Filter.listContains('transmitterReceiverPolarisation', 'VH'))\n",
    "      .filter(ee.Filter.eq('instrumentMode', 'IW')))\n",
    "\n",
    "# Sentinel-2 Level-2A surface reflectance\n",
    "# https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_S2_SR\n",
    "s2_sr_cld_col_eval = get_s2_sr_cld_col(start_date, end_date)\n",
    "s2 = s2_sr_cld_col_eval.map(add_cld_shdw_mask)\n",
    "\n",
    "# MODIS daily land surface temperature\n",
    "# https://developers.google.com/earth-engine/datasets/catalog/MODIS_006_MOD11A1\n",
    "# Temperature is given in Kelvin with a scaling factor of 0.02\n",
    "land_t = (ee.ImageCollection(\"MODIS/006/MOD11A1\")\n",
    "          .filterDate(start_date, end_date)\n",
    "          .select('LST_Day_1km'))\n",
    "\n",
    "# ERA-5 daily aggregate re-analysis data\n",
    "# https://developers.google.com/earth-engine/datasets/catalog/ECMWF_ERA5_DAILY\n",
    "# Maximum temperature (air) uses Kelvin\n",
    "# Precipitation is in meters\n",
    "era5 = (ee.ImageCollection('ECMWF/ERA5/DAILY')\n",
    "        .filterDate(start_date, end_date)\n",
    "        .select(['mean_2m_air_temperature', 'total_precipitation']))\n",
    "# Soil moisture data retrieved from ISMN\n",
    "# Retrieved from https://ismn.geo.tuwien.ac.at/en/\n",
    "root = r'C:\\Users\\Nick\\Documents\\_Master_Thesis\\Soil_Moisture\\ISMN'\n",
    "sm_data = ISMN_Interface(os.path.join(root, 'REMEDHUS.zip'), parallel=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataframe compiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compile the final merged dataframe of RS and SM data\n",
    "\n",
    "# Specify the probe network name\n",
    "# For multiple networks, these must be iterated through either through a script or by saving\n",
    "# a .csv for each network and then later merged\n",
    "network = 'SNOTEL'\n",
    "\n",
    "# Creates a soil moisture dataframe from the ISMN data\n",
    "# Specify the depth intervals (in meters) of the moisture measurements, e.g. 0.0, 0.05 for soil\n",
    "# moisture between 0-5cm\n",
    "# Set snow_depth=True to filter out days with snow cover if the data contains a snow depth variable\n",
    "df_sm, df_coords = get_sm_data(network, 0.0, 0.05, snow_depth=True)\n",
    "\n",
    "# Merge the soil moisture and remote sensing data for each station\n",
    "df = merge_sm_rs_data(df_sm, df_coords)\n",
    "\n",
    "# Export the dataframe to a local disk\n",
    "# Specify the out_dir with your local path\n",
    "out_dir = r'C:\\Users\\Nick\\Documents\\_Master_Thesis\\Soil_Moisture\\ISMN'\n",
    "df.to_csv(os.path.join(out_dir, 'SM_Data_{}.csv'.format(network)))\n",
    "print('Done!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
