{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preface\n",
    "This notebook runs on the Tensorflow (V2.4.0) GPU and is optimized for it. It can be run on a CPU but results in training times approximately x10 longer.\n",
    "Getting Tensorflow set up using a GPU can be a bit tricky if you're not familar with it but I strongly encourage you to spend the time and learn how. The time spent is saved in the long run if you a running on a proper GPU.\n",
    "\n",
    "See the documentation on how to set it up https://www.tensorflow.org/install/gpu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run this step if you are using a GPU\n",
    "## Prevents running out of GPU memory\n",
    "import tensorflow as tf\n",
    "\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.compat.v1.Session(config=config)\n",
    "\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dependencies and libraries\n",
    "Currently, more packages than necessary for the model to run are loaded. This is to allow the user to experiment with different types of RNN structures, such as GRU and Conv1D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## System functionality\n",
    "import os\n",
    "import glob\n",
    "from functools import reduce\n",
    "\n",
    "## Database and mathematical operations\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from random import sample\n",
    "\n",
    "## Machine learning\n",
    "# Tensorflow / Keras\n",
    "from tensorflow.keras import layers, optimizers, metrics\n",
    "from tensorflow.keras.layers import InputLayer, Conv1D, Dropout, LSTM, GRU, Dense\n",
    "from tensorflow.keras.regularizers import l1, l2, l1_l2\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import *\n",
    "# SKlearn\n",
    "from sklearn.model_selection import RandomizedSearchCV, TimeSeriesSplit\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "## Visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cyclical learning rate (see https://arxiv.org/abs/1506.01186)\n",
    "## Helps train the network faster and (sometimes) achieve better validation accuracy\n",
    "class CyclicLR(Callback):\n",
    "    def __init__(self, base_lr=0.001, max_lr=0.006, step_size=2000., mode='triangular',\n",
    "                gamma=1., scale_fn=None, scale_model='cycle'):\n",
    "        super(CyclicLR, self).__init__()\n",
    "        self.base_lr = base_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.step_size = step_size\n",
    "        self.mode = mode\n",
    "        self.gamma = gamma\n",
    "        if scale_fn == None:\n",
    "            if self.mode == 'triangular':\n",
    "                self.scale_fn = lambda x: 1.\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'triangular2':\n",
    "                self.scale_fn = lambda x: 1/(2.**(x-1))\n",
    "                self.scale_mode == 'cycle'\n",
    "            elif self.mode == 'exp_range':\n",
    "                self.scale_fn = lambda x: gamma**(x)\n",
    "                self.scale_mode = 'iterations'\n",
    "        else:\n",
    "            self.scale_fn = scale_fn\n",
    "            self.scale_mode = scale_mode\n",
    "        self.clr_iterations = 0\n",
    "        self.trn_iterations = 0\n",
    "        self.history = {}\n",
    "        self._reset()\n",
    "        \n",
    "    def _reset(self, new_base_lr=None, new_max_lr=None,\n",
    "              new_step_size=None):\n",
    "        # Resets cycle iterations\n",
    "        if new_base_lr != None:\n",
    "            self.base_lr = new_base_lr\n",
    "        if new_max_lr != None:\n",
    "            self.max_lr = new_max_lr\n",
    "        if new_step_size != None:\n",
    "            self.step_size = new_step_size\n",
    "        self.clr_iterations = 0.\n",
    "        \n",
    "    def clr(self):\n",
    "        cycle = np.floor(1+self.clr_iterations/(2*self.step_size))\n",
    "        x = np.abs(self.clr_iterations/self.step_size - 2*cycle + 1)\n",
    "        if self.scale_mode == 'cycle':\n",
    "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(cycle)\n",
    "        else:\n",
    "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(self.clr_iterations)\n",
    "        \n",
    "    def on_train_begin(self, logs={}):\n",
    "        logs = logs or {}\n",
    "        if self.clr_iterations == 0:\n",
    "            K.set_value(self.model.optimizer.lr, self.base_lr)\n",
    "        else:\n",
    "            K.set_value(self.model.optimizer.lr, self.clr())\n",
    "            \n",
    "    def on_batch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        self.trn_iterations += 1\n",
    "        self.clr_iterations += 1\n",
    "        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n",
    "        self.history.setdefault('iterations', []).append(self.trn_iterations)\n",
    "        \n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "            \n",
    "        K.set_value(self.model.optimizer.lr, self.clr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1200"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_date, end_date = '2017-03-28', '2020-07-09'\n",
    "date_index = pd.date_range(start=start_date, end=end_date, freq='D')\n",
    "len(date_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train/validation/test data split\n",
    "## This function uses dedicated validation and test probes and assigns the rest for training.\n",
    "## Variables, not including soil moisture, are normalised.\n",
    "def train_test_split(dataframe, val_probes, test_probes, start_date='2017-03-28',\n",
    "                     end_date='2020-07-09'):\n",
    "    df = dataframe.copy()\n",
    "    df = df.set_index('ID', append=True).sort_index(level=1)\n",
    "    train_probes = list(set(df.index.get_level_values('ID')))\n",
    "    \n",
    "    # Removes the validation and test probes from the dataframe\n",
    "    for probe in test_probes:\n",
    "        train_probes.remove(probe)\n",
    "    for probe in val_probes:\n",
    "        train_probes.remove(probe)\n",
    "    \n",
    "    date_index = pd.date_range(start=start_date, end=end_date, freq='D')\n",
    "    record_length = len(date_index)\n",
    "    \n",
    "    # Sorts the dataframes by date and ID\n",
    "    def sort_dataframe(dataframe, probes):\n",
    "        df_raw = dataframe.loc[pd.IndexSlice[:, probes], :]\n",
    "        df_raw = df_raw.reset_index(level='ID', col_level=1)\n",
    "        df_norm = pd.DataFrame()\n",
    "        for probe in probes:\n",
    "            df_ = df_raw[df_raw['ID'] == probe].copy()\n",
    "            df_ = df_[df_['soil_moisture'].notna()]\n",
    "            df_ = df_.reindex(index=date_index)\n",
    "            df_['ID'] = probe    \n",
    "            df_norm = df_norm.append(df_)\n",
    "        df_norm.index.name = 'Date'\n",
    "        df_norm.drop('ID', axis=1, inplace=True)\n",
    "        return df_norm\n",
    "        \n",
    "    train_df = sort_dataframe(df, train_probes) \n",
    "    val_df = sort_dataframe(df, val_probes) \n",
    "    \n",
    "    test_df = df.loc[pd.IndexSlice[:, test_probes], :].droplevel('ID') # copies the test probe data\n",
    "\n",
    "    train_target = (train_df.pop('soil_moisture').fillna(0).to_numpy()\n",
    "                    .reshape(len(train_probes), record_length))\n",
    "    val_target = (val_df.pop('soil_moisture').fillna(0).to_numpy()\n",
    "                  .reshape(len(val_probes), record_length))\n",
    "    test_target = (test_df.pop('soil_moisture').fillna(0).to_numpy()\n",
    "                  .reshape(len(test_probes), record_length))\n",
    "\n",
    "    # Normalizes the data \n",
    "    train_mean = train_df.mean()\n",
    "    train_std = train_df.std()\n",
    "    \n",
    "    train_df = (train_df - train_mean) / train_std\n",
    "    val_df = (val_df - train_mean) / train_std\n",
    "    test_df = (test_df - train_mean) / train_std\n",
    "\n",
    "    # Replacing NaNs with 0 for model interpretation\n",
    "    train_df.fillna(0, inplace=True)\n",
    "    val_df.fillna(0, inplace=True)\n",
    "    test_df.fillna(0, inplace=True)\n",
    "\n",
    "    # Converting the features to 3D numpy arrays\n",
    "    train_data = np.stack(np.split(train_df.to_numpy(), len(train_probes)))\n",
    "    val_data = np.stack(np.split(val_df.to_numpy(), len(val_probes)))\n",
    "    test_data = np.stack(np.split(test_df.to_numpy(), len(test_probes)))\n",
    "    \n",
    "    return (train_data, train_target, val_data, val_target, test_data, test_target, test_probes)\n",
    "\n",
    "def k_fold_split(dataframe, test_probes, start_date='2017-03-28',\n",
    "                 end_date='2020-07-09'):\n",
    "    \n",
    "\n",
    "# Returns information about the dimensions of the model data\n",
    "def data_stats(train_data):\n",
    "    samples = train_data.shape[0] * train_data.shape[1]\n",
    "    timesteps = train_data.shape[1]\n",
    "    features = train_data.shape[2]\n",
    "    batch_length = int(samples / timesteps)\n",
    "    return (samples, timesteps, features, batch_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(history, metric)\n",
    "    training_metric = history.history[metric]\n",
    "    validation_metric = history.history['val_{}'.format(metric)]\n",
    "\n",
    "    epochs = range(1, len(training_metric) + 1)\n",
    "\n",
    "    fig = plt.figure(figsize=(12, 6), dpi=100, facecolor='w', edgecolor='k')\n",
    "    ax = fig.add_subplot()\n",
    "    ax.set_facecolor('#f7f7f7')\n",
    "\n",
    "    plt.plot(epochs, training_metric, 'b+', label='Training {}'.format(metric))\n",
    "    plt.plot(epochs, validation_metric, 'royalblue', label='Validation {}'.format(metric))\n",
    "    plt.title('Model training and validation {}'.format(metric), fontsize=16)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel(metric)\n",
    "\n",
    "    plt.legend()\n",
    "    plt.grid(axis='y')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
