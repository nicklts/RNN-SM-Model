{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preface\n",
    "This notebook runs on the Tensorflow (V2.5.0) GPU and is optimized for it. It can be run on a CPU but results in training times approximately x10 longer.\n",
    "Getting Tensorflow set up using a GPU can be a bit tricky if you're not familar with it but I strongly encourage you to spend the time and learn how. The time spent is saved in the long run if you a running on a proper GPU.\n",
    "\n",
    "See the documentation on how to set it up https://www.tensorflow.org/install/gpu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run this step if you are using a GPU\n",
    "## Prevents running out of GPU memory\n",
    "import tensorflow as tf\n",
    "\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.compat.v1.Session(config=config)\n",
    "\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dependencies and libraries\n",
    "Currently, more packages than necessary for the model to run are loaded. This is to allow the user to experiment with different types of RNN structures, such as GRU and Conv1D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## System functionality\n",
    "import os\n",
    "import glob\n",
    "from functools import reduce\n",
    "from datetime import datetime\n",
    "\n",
    "## Database and mathematical operations\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from random import sample\n",
    "\n",
    "## Machine learning\n",
    "# Tensorflow / Keras\n",
    "from tensorflow.keras import layers, optimizers, metrics\n",
    "from tensorflow.keras.layers import InputLayer, Conv1D, Dropout, LSTM, GRU, Dense\n",
    "from tensorflow.keras.regularizers import l1, l2, l1_l2\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import *\n",
    "# SKlearn\n",
    "from sklearn.model_selection import RandomizedSearchCV, TimeSeriesSplit\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "## Visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cyclical learning rate (see https://arxiv.org/abs/1506.01186)\n",
    "## Helps train the network faster and (sometimes) achieve better validation accuracy\n",
    "class CyclicLR(Callback):\n",
    "    def __init__(self, base_lr=0.001, max_lr=0.006, step_size=2000., mode='triangular',\n",
    "                gamma=1., scale_fn=None, scale_model='cycle'):\n",
    "        super(CyclicLR, self).__init__()\n",
    "        self.base_lr = base_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.step_size = step_size\n",
    "        self.mode = mode\n",
    "        self.gamma = gamma\n",
    "        if scale_fn == None:\n",
    "            if self.mode == 'triangular':\n",
    "                self.scale_fn = lambda x: 1.\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'triangular2':\n",
    "                self.scale_fn = lambda x: 1/(2.**(x-1))\n",
    "                self.scale_mode == 'cycle'\n",
    "            elif self.mode == 'exp_range':\n",
    "                self.scale_fn = lambda x: gamma**(x)\n",
    "                self.scale_mode = 'iterations'\n",
    "        else:\n",
    "            self.scale_fn = scale_fn\n",
    "            self.scale_mode = scale_mode\n",
    "        self.clr_iterations = 0\n",
    "        self.trn_iterations = 0\n",
    "        self.history = {}\n",
    "        self._reset()\n",
    "        \n",
    "    def _reset(self, new_base_lr=None, new_max_lr=None,\n",
    "              new_step_size=None):\n",
    "        # Resets cycle iterations\n",
    "        if new_base_lr != None:\n",
    "            self.base_lr = new_base_lr\n",
    "        if new_max_lr != None:\n",
    "            self.max_lr = new_max_lr\n",
    "        if new_step_size != None:\n",
    "            self.step_size = new_step_size\n",
    "        self.clr_iterations = 0.\n",
    "        \n",
    "    def clr(self):\n",
    "        cycle = np.floor(1+self.clr_iterations/(2*self.step_size))\n",
    "        x = np.abs(self.clr_iterations/self.step_size - 2*cycle + 1)\n",
    "        if self.scale_mode == 'cycle':\n",
    "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(cycle)\n",
    "        else:\n",
    "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(self.clr_iterations)\n",
    "        \n",
    "    def on_train_begin(self, logs={}):\n",
    "        logs = logs or {}\n",
    "        if self.clr_iterations == 0:\n",
    "            K.set_value(self.model.optimizer.lr, self.base_lr)\n",
    "        else:\n",
    "            K.set_value(self.model.optimizer.lr, self.clr())\n",
    "            \n",
    "    def on_batch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        self.trn_iterations += 1\n",
    "        self.clr_iterations += 1\n",
    "        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n",
    "        self.history.setdefault('iterations', []).append(self.trn_iterations)\n",
    "        \n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "            \n",
    "        K.set_value(self.model.optimizer.lr, self.clr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train/validation/test data split\n",
    "## This function uses dedicated validation and test probes and assigns the rest for training.\n",
    "## Variables, not including soil moisture, are normalised.\n",
    "def train_test_split(dataframe, val_probes, test_probes, start_date='2017-03-28',\n",
    "                     end_date='2020-07-09'):\n",
    "    df = dataframe.copy()\n",
    "    df = df.set_index('ID', append=True).sort_index(level=1)\n",
    "    train_probes = list(set(df.index.get_level_values('ID')))\n",
    "    \n",
    "    # Removes the validation and test probes from the dataframe\n",
    "    for probe in test_probes:\n",
    "        train_probes.remove(probe)\n",
    "    for probe in val_probes:\n",
    "        train_probes.remove(probe)\n",
    "    \n",
    "    date_index = pd.date_range(start=start_date, end=end_date, freq='D')\n",
    "    record_length = len(date_index)\n",
    "    \n",
    "    # Sorts the dataframes by date and ID\n",
    "    # This function ensures that all probes have an equal number of timesteps and that \n",
    "    def sort_dataframe(dataframe, probes):\n",
    "        df_raw = dataframe.loc[pd.IndexSlice[:, probes], :]\n",
    "        df_raw = df_raw.reset_index(level='ID', col_level=1)\n",
    "        df_norm = pd.DataFrame()\n",
    "        for probe in probes:\n",
    "            df_ = df_raw[df_raw['ID'] == probe].copy()\n",
    "            df_ = df_[df_['soil_moisture'].notna()]\n",
    "            df_ = df_.reindex(index=date_index)\n",
    "            df_['ID'] = probe    \n",
    "            df_norm = df_norm.append(df_)\n",
    "        df_norm.index.name = 'Date'\n",
    "        df_norm.drop('ID', axis=1, inplace=True)\n",
    "        return df_norm\n",
    "        \n",
    "    train_df = sort_dataframe(df, train_probes) \n",
    "    val_df = sort_dataframe(df, val_probes) \n",
    "    \n",
    "    test_df = df.loc[pd.IndexSlice[:, test_probes], :].droplevel('ID') # copies the test probe data\n",
    "\n",
    "    # Removes the targets (soil moisture) from the data before normalization\n",
    "    train_target = (train_df.pop('soil_moisture').fillna(0).to_numpy()\n",
    "                    .reshape(len(train_probes), record_length))\n",
    "    val_target = (val_df.pop('soil_moisture').fillna(0).to_numpy()\n",
    "                  .reshape(len(val_probes), record_length))\n",
    "    test_target = (test_df.pop('soil_moisture').fillna(0).to_numpy()\n",
    "                  .reshape(len(test_probes), record_length))\n",
    "\n",
    "    # Normalizes the data \n",
    "    train_mean = train_df.mean()\n",
    "    train_std = train_df.std()\n",
    "    \n",
    "    train_df = (train_df - train_mean) / train_std\n",
    "    val_df = (val_df - train_mean) / train_std\n",
    "    test_df = (test_df - train_mean) / train_std\n",
    "\n",
    "    # Replacing NaNs with 0 for model interpretation\n",
    "    train_df.fillna(0, inplace=True)\n",
    "    val_df.fillna(0, inplace=True)\n",
    "    test_df.fillna(0, inplace=True)\n",
    "\n",
    "    # Converting the features to 3D numpy arrays\n",
    "    train_data = np.stack(np.split(train_df.to_numpy(), len(train_probes)))\n",
    "    val_data = np.stack(np.split(val_df.to_numpy(), len(val_probes)))\n",
    "    test_data = np.stack(np.split(test_df.to_numpy(), len(test_probes)))\n",
    "    \n",
    "    return (train_data, train_target, val_data, val_target, test_data, test_target)\n",
    "\n",
    "def k_fold_split(dataframe, test_probes, start_date='2017-03-28',\n",
    "                 end_date='2020-07-09'):\n",
    "    return 'test'\n",
    "    \n",
    "\n",
    "# Returns information about the dimensions of the model data\n",
    "def data_stats(train_data):\n",
    "    samples = train_data.shape[0] * train_data.shape[1]\n",
    "    timesteps = train_data.shape[1]\n",
    "    features = train_data.shape[2]\n",
    "    batch_length = int(samples / timesteps)\n",
    "    return (samples, timesteps, features, batch_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(history, metric):\n",
    "    training_metric = history.history[metric]\n",
    "    validation_metric = history.history['val_{}'.format(metric)]\n",
    "\n",
    "    epochs = range(1, len(training_metric) + 1)\n",
    "\n",
    "    fig = plt.figure(figsize=(12, 6), dpi=100, facecolor='w', edgecolor='k')\n",
    "    ax = fig.add_subplot()\n",
    "    ax.set_facecolor('#f7f7f7')\n",
    "\n",
    "    plt.plot(epochs, training_metric, 'b+', label='Training {}'.format(metric))\n",
    "    plt.plot(epochs, validation_metric, 'royalblue', label='Validation {}'.format(metric))\n",
    "    plt.title('Model training and validation {}'.format(metric), fontsize=16)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel(metric)\n",
    "\n",
    "    plt.legend()\n",
    "    plt.grid(axis='y')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Local directories\n",
    "Specify the local directory of your machine where the soil moisture .csv file is kept for **root_dir**.\n",
    "\n",
    "#### Data formatting\n",
    "The **test_train_split** function splits the dataframe into training, validation, and testing. Specify the stations used for validation and testing in arrays like ['station1', 'station2', etc.]\n",
    "\n",
    "The **data_stats** function returns the dimensions of the training data, which is required for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local parent folder of the soil moisture file\n",
    "root_dir = r'C:\\Users\\Nick\\Documents\\_Master_Thesis\\Soil_Moisture\\ISMN'\n",
    "\n",
    "# You may or may not need the date_parser argument to load the dates correctly.\n",
    "# I recommend running the function first with the date_parser but if it throws you an error,\n",
    "# just comment it out.\n",
    "date_parse = lambda x: datetime.strptime(x, '%d/%m/%Y')\n",
    "df = pd.read_csv(os.path.join(root_dir, 'SM_Data_REMEDHUS.csv'), index_col='Date', parse_dates=True,\n",
    "                date_parser=date_parse)\n",
    "\n",
    "# Returns normalized data split into training, validation, and testing arrays\n",
    "# Specify the validation and test probes as arrays\n",
    "X_train, Y_train, X_val, Y_val, X_test, Y_test = train_test_split(df, ['Canizal'], ['Carretoro'])\n",
    "\n",
    "# Returns the dimensions of the data\n",
    "samples, timesteps, features, batch_length = data_stats(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep learning model - Recurrent Neural Network\n",
    "*Note that some hyperparameters, such as recurrent_dropout, are not available when training with a GPU.*\n",
    "\n",
    "#### Hyperparameters\n",
    "Both **clr** and **l2_decay** are hyperparameters of the model, which can/should be tweaked between model runs. For **clr** see https://arxiv.org/abs/1506.01186 for a description about what the function does and how each parameter affects it.\n",
    "\n",
    "The **epochs** are specified in the last block during the model.fit. This can/should be modified depending on the data. Larger datasets require more epochs, while a large number of epochs will result in overfitting on small datasets.\n",
    "\n",
    "The **loss function**, **optimizer**, and **metrics** are used to assess the accuracy of the model during training and modify the weights accordingly. You should experiment with different types of optimizers (see https://keras.io/api/optimizers/) as there is no general rule for which is the better one, it varies between datasets. These are specified before fitting the model in model.compile.  \n",
    "\n",
    "#### Layers\n",
    "Finding the right hyperparameters are certainly important but the largest impact on the model will come from the number of layers used, the type, and the size of each layer. You should experiment with different types of layers (see https://keras.io/api/layers/recurrent_layers/ and https://keras.io/api/layers/convolution_layers/convolution1d/) and different sizes for the layers.\n",
    "\n",
    "Following the layers in the default setup is a dropout layer, which disables random neurons in the model to reduce overfitting. This may or may not be advantageous for all datasets so experiment with different ratios (typically between 0.1 and 0.2) or disable them completely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization and regularization hyperparameters\n",
    "clr = CyclicLR(base_lr=1e-6, max_lr=0.002, step_size=250., mode='triangular', gamma=0.99995)\n",
    "l2_decay = 1e-4\n",
    "\n",
    "# Initializes the model and describes the dimensions of the data with the input_layer.\n",
    "# This section should not be changed\n",
    "model = Sequential()\n",
    "model.add(InputLayer(input_shape=(timesteps, features)))\n",
    "\n",
    "# An LSTM layer with recurrent regularization followed by a dropout layer\n",
    "model.add(LSTM(8, return_sequences=True, recurrent_regularizer=l2(l2_decay)))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "# An LSTM layer with recurrent regularization followed by a dropout layer\n",
    "model.add(LSTM(8, return_sequences=True, recurrent_regularizer=l2(l2_decay)))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "# A dense layer with a single node to fit the data back a desired format\n",
    "# This should not be changed or removed\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Compiles the model with the specified loss function, optimizer, and metrics.\n",
    "# See the Keras/TF documentation for a description what the hyperparameters do and which other\n",
    "# types can be used https://www.tensorflow.org/api_docs/python/tf/keras/Model\n",
    "model.compile(loss=tf.keras.losses.MeanAbsoluteError(),\n",
    "             optimizer='Adam',\n",
    "             metrics=[metrics.MeanAbsoluteError()])\n",
    "\n",
    "# Fits the compiled model using the training and validation data\n",
    "# Implements any callback functions specified\n",
    "history = model.fit(X_train, Y_train,\n",
    "                    batch_size=1,\n",
    "                    epochs=20,\n",
    "                    callbacks=[clr, ],\n",
    "                    validation_data=(X_val, Y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots the results from the model\n",
    "# The available metrics that can be plotted depends on which metrics were used during model\n",
    "# training. For the list of available metrics and their validation equivalent, uncomment and \n",
    "# run the function below\n",
    "# print(history.history.keys())\n",
    "\n",
    "plot_results(history, 'mean_absolute_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluates the model on the test data\n",
    "prediction = model.evaluate(X_test, Y_test, batch_size=1)\n",
    "print('Mean Squared Error: {}\\nMean Absolute Error: {}%'\n",
    "      .format(prediction[0], round(prediction[1]*100, 2)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
